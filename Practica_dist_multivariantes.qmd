---
title: "Distribuciones multivariantes en práctica: Normal y familias relacionadas"
date: "`r format(Sys.Date())`"
autor: Paula G Ferrer, Pep Cifre i Marc Escandell
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
  pdf_document: default
fontsize: 11pt
geometry: margin=1in
embed-resources: true
editor: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library("tidyverse")
library("palmerpenguins")
library("GGally")
library("MVN")
```

Considera el conjunto de datos
[palmerpenguins](https://allisonhorst.github.io/palmerpenguins/), leed
la documentación, instalad y cargad el conjunto de datos penguins

![](pinguinos.png){width="40%" style="display:block; margin:auto;"}

Cargamos las librerías, leemos los datos y los resumimos por especie:

```{r data}
library(tidyverse)
library(palmerpenguins)

pinguinos <- palmerpenguins::penguins %>%
  select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) 
#View(pinguinos)

datosPinguinos <- pinguinos %>% # selecionamos los datos numericos del data frame
  select(-species) %>% drop_na()
#View(datosPinguinos)
 
  
pinguinos %>%
  group_by(species) %>%
  summarise(
    n = n(),
    across(where(is.numeric), ~ mean(.x, na.rm = TRUE)),
    .groups = "drop"
  )
```

Vamos a trabajar con la especie "Adelie" que es donde tenemos más datos

```{r seleccion-especie}
A <- pinguinos %>% filter(species == "Adelie") %>% select(-species) %>%
  drop_na()
n <- nrow(A); p <- ncol(A)
n; p
```

## Análisis descriptivo multivariante

```{r resumen}
mu <- colMeans(A)
S  <- cov(A)

round(mu, 2)
round(S, 2)
```

```{r pairs}
GGally::ggpairs(A)
```

### 1. Escribe en 3–4 líneas las asociaciones más claras. ¿Tiene sentido suponer relaciones aproximadamente lineales? ¿Las densidades marginales lucen gaussianas, eso garantiza normalidad multivariante?

```{r}
# (X,Y) con X ~ N(0,1) y Y = ±X con prob 1/2

set.seed(123)

n <- 2e4
x <- rnorm(n)
s <- sample(c(-1, 1), n, replace = TRUE)
y <- s * x
df <- data.frame(x, y)

p1 <- ggplot(df, aes(x, y)) +
  geom_point(alpha = 0.25, shape = 16, size = 0.7) +
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  geom_abline(slope = -1, intercept = 0, linetype = 2) +
  coord_equal() +
  labs(title = "Densidad conjunta del contraejemplo")

p1
```

La densidad conjunta es\

$$
f_{X,Y}(x,y)=\tfrac12\phi(y)\delta(y-x)+\tfrac12\phi(y)\delta(y+x),
$$ donde $\phi$ es la densidad normal estándar y $\delta$ es la delta de
Dirac.

(Completa los detalles !)

**Recuerda:** En una normal multivariante:

-   Las curvas de nivel son elipses en 2D (elipsoides en dimensiones
    mayores).
-   La dependencia entre variables queda completamente determinada por
    la matriz de covarianzas $\Sigma$.

Mirar solo las marginales puede ocultar dependencias no lineales o
multimodales que rompen la normalidad conjunta.

#### Possible resposta:

Las especias más grandes (como gentoo) presentan mayores logitudes de
aleta y masa corportal, mientras que Adelie tiende a valores menores.
Correlacion positiva entre longitud del pico, longitud aleta y masa
corporal, además de significativas p-valor\<0.001. Destaca que la
profundidad del pico se comporta de manera diferente, separando especies
más por su forma que por tamaño general. En los diagramas de dispersión,
las relaciones parecen aproximadamente "lineales". #AÑADIR GRAFICO CORR
con todas las especies o cada uno por separado

Si consideramos $Y=+-X$, y definimos que $X$ sigue una $N(0,1)$, con
$S\in \{-1,1\}$ y $P(S=1)=P(S=-1)=1/2$, tendríamos que $Y=SX$.

Veamos que la función de densidad de $Y$ es una normal $N(0,1)$. Para ello, calculemos su función de distribución.

$$F_Y(y) = P\{Y \leq y\} = P\{SX \leq y \} = P\{SX \leq y \mid S = 1\}P\{S=1\} + P\{SX \leq y \mid S = -1\}P\{S=-1\}$$
Donde hemos aplicado el teorema de la probabilidad total. De esta forma,

$$F_Y(y) = \frac{1}{2}(P\{X \leq y\} + P\{-X \leq y\}) = \frac{1}{2}(P\{X \leq y\} + 1 - P\{X \leq -y\}) = \frac{1}{2}(F_X(y) + 1 - F_X(-y))$$
Per tant, derivant l'expressió anterior tenim que:

$$f_Y(y) = \frac{1}{2}(f_X(y) + f_X(-y)) = f_X(y)$$
Debido a que $X$ sigue una normal estándard y por tanto es simétrica. Por tanto, $Y$ sigue una normal $N(0, 1)$. Ahora bien, el vector conjunto no tiene una función de densidad elíptica, por tanto no habrá normalidad.

### 2. Interpreta dos términos fuera de la diagonal de $\Sigma$. ¿Qué nos dicen sobre la orientación de las elipses de nivel?

Pos resposta: $\Sigma_{ij} = Cov(X_i,X_j)=A$ son els elements fora de sa
diagonal. Llavors,

-   Si A\>0, quan una variable creix s'altre tambe

-   A\<0, una creix s'altre baix

-   A=0, no estan correlaciones (no te perque ser indepes)

### 3. Utiliza las distancias de Mahalanobis para detectar datos atípicos multivariantes. ¿Los puntos señalados como atípicos lo son también en sentido univariante? Compara con boxplots univariantes.

La distancia de Mahalanobis mide lo “lejos” que está un punto del centro
considerando las escalas y correlaciones entre variables. Para un vector
de observaciones $\mathbf{x}\in\mathbb{R}^p$ con media muestral
$\boldsymbol{\mu}$ y matriz de covarianzas $\mathbf{S}$, se define
$$D^2(\mathbf{x}) = (\mathbf{x}-\boldsymbol{\mu})^\top \mathbf{S}^{-1} (\mathbf{x}-\boldsymbol{\mu})$$.

Si los datos proceden de una normal multivariante
$N_p(\boldsymbol{\mu},\mathbf{S})$, entonces
$D^2(\mathbf{x}) \sim \chi^2_p$. Por tanto, valores grandes de $D^2$
indican posibles atípicos en el sentido multivariante.

*Regla práctica:*

Para cada observación $i$, calcula $D_i^2$ y compárala con el cuantil
crítico: $D_i^2 > \chi^2_{p,\,1-\alpha} \;\Rightarrow\;$ marcar como
candidato a atípico (con $\alpha$ típico $0.05$ o $0.01$).

La distancia de Mahalanobis:

-   Estandariza por varianzas (no se ve afectada por escalas distintas),

-   Corrige por correlaciones vía $\mathbf{S}^{-1}$, y

-   Genera elipses de nivel $\{\mathbf{x} : D^2(\mathbf{x}) = c\}$
    coherentes con la geometría de $N_p$.

Por eso detecta observaciones que pueden ser normales univariadamente
pero atípicas al considerar el vector completo.

repsosta: Los conjuntos de igual distancia D\^2=c, son elipses o
elipsoides. La distancia de Mahalanobis define niveles de densidad
constante en la normal multivariente. Un punto puede parecer normal
univariante pero atipico al considerar la combinacion conjunta de
variables

```{r}
library(tidyverse)
library(palmerpenguins)

# Cargamos los datos y seleccionamos solo las variables numéricas
pinguinos <- penguins %>%
  select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)

# Eliminamos los NA
pinguinos_complete <- na.omit(pinguinos)

# Calculamos las distancias de Mahalanobis
mu <- colMeans(pinguinos_complete[, -1])  # sin la especie
S <- cov(pinguinos_complete[, -1])
D2 <- mahalanobis(pinguinos_complete[, -1], center = mu, cov = S)

p1<-ncol(pinguinos_complete)
cutoff <- qchisq(0.95,df=p1)
outliers<-which(D2>cutoff)
outliers


pinguinos_complete <- pinguinos_complete %>%
  mutate(D2 = D2)

p <- ncol(pinguinos_complete) - 1  # quitamos la especie
cutoff <- qchisq(0.95, df = p)
pinguinos_complete <- pinguinos_complete %>%
  mutate(atipico = D2 > cutoff)


pinguinos_long <- pinguinos_complete %>%
  select(-D2, -atipico) %>%
  pivot_longer(
    cols = where(is.numeric),
    names_to = "variable",
    values_to = "valor"
  )


ggplot(pinguinos_long, aes(x = variable, y = valor)) +
  geom_boxplot(aes(fill = variable)) +
  theme_minimal() +
  labs(
    title = "Boxplots univariantes de las variables morfológicas",
    x = "Variable",
    y = "Valor"
  ) +
  theme(legend.position = "none")

```

### 4. Evalúa la normalidad multivariante : tests y QQ-plots. ¿Observas curvaturas sistemáticas (no linealidad global) o solo unos pocos puntos extremos?

En cuanto a los tests, intenta usar la librería: `MVN` prueba "mardia",
"hz" y "royston" ¿en qué se basan cada uno?.

En cuanto al QQ-plot, te dejo un pseudo algoritmo para que lo
implementes:

```{r}
library(MVN)
#d2 <- mahalanobis(datos, mu, S)
P <- as.matrix(datosPinguinos)
# mu vector de medias i S matriz de covariancias
mu <- colMeans(P)
S <- cov(P)

d2 <- mahalanobis(datosPinguinos, mu, S)

qqplot(qchisq(ppoints(n), df = p), sort(d2),
main = "QQ-plot de distancias de Mahalanobis",
xlab = expression("Cuantiles teóricos" ~ chi^2[p]),
ylab = expression("Distancias de Mahalanobis" ~ D^2))
abline(0, 1, col = "red", lwd = 2)

```

-   Si los puntos caen cerca de la recta: la normalidad multivariante es
    plausible.

-   Si hay una curvatura clara o puntos extremos: hay desviaciones o
    outliers multivariantes. En ese caso, vuelve a evaluar la normalidad
    tras excluirlos.

```{r}
library(MVN)
library(tidyverse)
library(palmerpenguins)

# datos
pinguinos <- penguins %>%
  select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%
  na.omit()

# R ME ODIA
# Evaluamos la normalidad con los tres tests
mardia_test  <- mvn(data = pinguinos, mvnTest = "mardia", multivariatePlot = "qq")
hz_test      <- mvn(data = pinguinos, mvnTest = "hz", multivariatePlot = "qq")
royston_test <- mvn(data = pinguinos, mvnTest = "royston", multivariatePlot = "qq")

# Mostramos los resultados
mardia_test$multivariateNormality
hz_test$multivariateNormality
royston_test$multivariateNormality

# QQ-plot de distancias de Mahalanobis
P <- as.matrix(pinguinos)
mu <- colMeans(P)
S <- cov(P)
n <- nrow(P)
p <- ncol(P)

d2 <- mahalanobis(P, mu, S)

qqplot(qchisq(ppoints(n), df = p), sort(d2),
       main = "QQ-plot de distancias de Mahalanobis",
       xlab = expression("Cuantiles teóricos" ~ chi^2[p]),
       ylab = expression("Distancias de Mahalanobis" ~ D^2))
abline(0, 1, col = "red", lwd = 2)

```

### 5. Comparación entre especies: Superpón elipses normales por especie (elige dos variables) y comenta diferencias de centro y forma/orientación:

resposta: Si por ejemplo elegimos (X1,X2) = (long pico, long aleta)

```{r}
library(tidyverse)
library(palmerpenguins)

# Selección de dos variables
pinguinos <- penguins %>%
  select(species, bill_length_mm, flipper_length_mm) %>%
  drop_na()

#  Gráfico con elipses normales por especie
ggplot(pinguinos, aes(x = bill_length_mm, y = flipper_length_mm, color = species)) +
  geom_point(alpha = 0.6) +
  stat_ellipse(type = "norm", level = 0.95, size = 1) +
  theme_minimal() +
  labs(
    title = "Elipses normales (95%) por especie de pingüino",
    x = "Longitud del pico (mm)",
    y = "Longitud de la aleta (mm)",
    color = "Especie"
  ) +
  theme(plot.title = element_text(face = "bold", size = 13))

```

### 6. ¿Parece razonable asumir una $\Sigma$ común entre especies? Justifica con evidencia gráfica.

### 7. ¿Podemos afirmar que la matriz $\mathbf{S}$ de los pinguinos Adelie sigue una distribución de Wishart?

### 8. Comprueba por simulación que $n\mathbf{S}$, obtenida a partir de muestras de una normal multivariante, sigue aproximadamente una distribución de Wishart. ¿Coinciden las medias empíricas de los elementos de $n\mathbf{S}$ con los valores esperados según $n\boldsymbol{\Sigma}$? ¿Qué ocurre si se aumenta o reduce $n$? ¿Y si se cambia la matriz $\boldsymbol{\Sigma}$?

### 9. Genera una muestra de tamaño $n$ y dimensión $p$ de una distribución normal multivariante. Calcula el estadístico de Hotelling $T^2$ y transforma su valor a la escala de una F mediante la relación conocida entre ambas distribuciones. Explora cómo varía el valor de $T^2$ al modificar: el tamaño muestral $n$,la dimensión $p$,la distancia entre el vector de medias y un valor de referencia $\boldsymbol{\mu}_0$. Representa gráficamente el comportamiento de $T^2$ o de su equivalente en la escala F cuando cambias estos parámetros.

### 10. Simula ahora varios grupos con medias distintas pero con la misma matriz de covarianzas, y calcula el valor de Wilks $\Lambda$ y su transformación a F. Usa el siguiente código como base:

```{r}
library(MASS)
# Ejemplo con 3 grupos simulados
set.seed(123)
g <- 3; n <- 25; p <- 2
Sigma <- matrix(c(1, 0.6, 0.6, 1), 2)
mu_list <- list(c(0,0), c(1,0.5), c(0.5,1))
X <- do.call(rbind, lapply(1:g, function(k) mvrnorm(n, mu_list[[k]], Sigma)))
grupo <- factor(rep(1:g, each = n))
D <- data.frame(grupo, X)

# MANOVA (acrónimo de Multivariate Analysis of Variance) es la extensión multivariante del ANOVA
modelo <- manova(cbind(X1, X2) ~ grupo, data = D)
summary(modelo, test = "Wilks")
```

##### 10a. Calcula manualmente las matrices $\mathbf{W}$, $\mathbf{B}$ y $\mathbf{T}$, y el cociente $\Lambda = |\mathbf{W}| / |\mathbf{T}|$.

##### 10b. Comprueba que la F obtenida en la salida de summary(modelo, test = "Wilks") coincide con la transformación aproximada de $\Lambda$.

##### 10c. Repite la simulación con medias más separadas y observa cómo varían $\Lambda$ y F.
