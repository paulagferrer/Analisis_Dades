---
title: "Distribuciones multivariantes en práctica: Normal y familias relacionadas"
date: "`r format(Sys.Date())`"
autor: Paula G Ferrer, Pep Cifre i Marc Escandell
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
  pdf_document: default
fontsize: 11pt
geometry: margin=1in
embed-resources: true
editor: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library("tidyverse")
library("palmerpenguins")
library("GGally")
library("MVN")
```

Considera el conjunto de datos
[palmerpenguins](https://allisonhorst.github.io/palmerpenguins/), leed
la documentación, instalad y cargad el conjunto de datos penguins

![](pinguinos.png){width="40%" style="display:block; margin:auto;"}

Cargamos las librerías, leemos los datos y los resumimos por especie:

```{r data}
library(tidyverse)
library(palmerpenguins)

pinguinos <- palmerpenguins::penguins %>%
  select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) 
#View(pinguinos)

datosPinguinos <- pinguinos %>% # selecionamos los datos numericos del data frame
  select(-species)
#View(datosPinguinos)
datosPinguinos %>% drop_na()
  
pinguinos %>%
  group_by(species) %>%
  summarise(
    n = n(),
    across(where(is.numeric), ~ mean(.x, na.rm = TRUE)),
    .groups = "drop"
  )
```

Vamos a trabajar con la especie "Adelie" que es donde tenemos más datos

```{r seleccion-especie}
A <- pinguinos %>% filter(species == "Adelie") %>% select(-species) %>%
  drop_na()
n <- nrow(A); p <- ncol(A)
n; p
```

## Análisis descriptivo multivariante

```{r resumen}
mu <- colMeans(A)
S  <- cov(A)

round(mu, 2)
round(S, 2)
```

```{r pairs}
GGally::ggpairs(A)
```

### 1. Escribe en 3–4 líneas las asociaciones más claras. ¿Tiene sentido suponer relaciones aproximadamente lineales? ¿Las densidades marginales lucen gaussianas, eso garantiza normalidad multivariante?

```{r}
# (X,Y) con X ~ N(0,1) y Y = ±X con prob 1/2

set.seed(123)

n <- 2e4
x <- rnorm(n)
s <- sample(c(-1, 1), n, replace = TRUE)
y <- s * x
df <- data.frame(x, y)

p1 <- ggplot(df, aes(x, y)) +
  geom_point(alpha = 0.25, shape = 16, size = 0.7) +
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  geom_abline(slope = -1, intercept = 0, linetype = 2) +
  coord_equal() +
  labs(title = "Densidad conjunta del contraejemplo")

p1
```

La densidad conjunta es\

$$
f_{X,Y}(x,y)=\tfrac12\phi(y)\delta(y-x)+\tfrac12\phi(y)\delta(y+x),
$$
donde $\phi$ es la densidad normal estándar y $\delta$ es la delta de
Dirac.

(Completa los detalles !)

**Recuerda:** En una normal multivariante:

-   Las curvas de nivel son elipses en 2D (elipsoides en dimensiones
    mayores).
-   La dependencia entre variables queda completamente determinada por
    la matriz de covarianzas $\Sigma$.

Mirar solo las marginales puede ocultar dependencias no lineales o
multimodales que rompen la normalidad conjunta.

### 2. Interpreta dos términos fuera de la diagonal de $\Sigma$. ¿Qué nos dicen sobre la orientación de las elipses de nivel?

### 3. Utiliza las distancias de Mahalanobis para detectar datos atípicos multivariantes. ¿Los puntos señalados como atípicos lo son también en sentido univariante? Compara con boxplots univariantes.

La distancia de Mahalanobis mide lo “lejos” que está un punto del centro
considerando las escalas y correlaciones entre variables. Para un vector
de observaciones $\mathbf{x}\in\mathbb{R}^p$ con media muestral
$\boldsymbol{\mu}$ y matriz de covarianzas $\mathbf{S}$, se define
$$D^2(\mathbf{x}) = (\mathbf{x}-\boldsymbol{\mu})^\top \mathbf{S}^{-1} (\mathbf{x}-\boldsymbol{\mu})$$.

Si los datos proceden de una normal multivariante
$N_p(\boldsymbol{\mu},\mathbf{S})$, entonces
$D^2(\mathbf{x}) \sim \chi^2_p$. Por tanto, valores grandes de $D^2$
indican posibles atípicos en el sentido multivariante.

*Regla práctica:*

Para cada observación $i$, calcula $D_i^2$ y compárala con el cuantil
crítico: $D_i^2 > \chi^2_{p,\,1-\alpha} \;\Rightarrow\;$ marcar como
candidato a atípico (con $\alpha$ típico $0.05$ o $0.01$).

La distancia de Mahalanobis:

-   Estandariza por varianzas (no se ve afectada por escalas distintas),

-   Corrige por correlaciones vía $\mathbf{S}^{-1}$, y

-   Genera elipses de nivel $\{\mathbf{x} : D^2(\mathbf{x}) = c\}$
    coherentes con la geometría de $N_p$.

Por eso detecta observaciones que pueden ser normales univariadamente
pero atípicas al considerar el vector completo.

### 4. Evalúa la normalidad multivariante : tests y QQ-plots. ¿Observas curvaturas sistemáticas (no linealidad global) o solo unos pocos puntos extremos?

En cuanto a los tests, intenta usar la librería: `MVN` prueba "mardia",
"hz" y "royston" ¿en qué se basan cada uno?.

En cuanto al QQ-plot, te dejo un pseudo algoritmo para que lo
implementes:

```{r}
#d2 <- mahalanobis(datos, mu, S)
P <- as.matrix(datosPinguinos)
# mu vector de medias i S matriz de covariancias
mu <- colMeans(P)
S <- cov(P)

d2 <- mahalanobis(datosPinguinos, mu, S)

qqplot(qchisq(ppoints(n), df = p), sort(d2),
main = "QQ-plot de distancias de Mahalanobis",
xlab = expression("Cuantiles teóricos" ~ chi^2[p]),
ylab = expression("Distancias de Mahalanobis" ~ D^2))
abline(0, 1, col = "red", lwd = 2)

```

-   Si los puntos caen cerca de la recta: la normalidad multivariante es
    plausible.

-   Si hay una curvatura clara o puntos extremos: hay desviaciones o
    outliers multivariantes. En ese caso, vuelve a evaluar la normalidad
    tras excluirlos.

### 5. Comparación entre especies: Superpón elipses normales por especie (elige dos variables) y comenta diferencias de centro y forma/orientación:

### 6. ¿Parece razonable asumir una $\Sigma$ común entre especies? Justifica con evidencia gráfica.

### 7. ¿Podemos afirmar que la matriz $\mathbf{S}$ de los pinguinos Adelie sigue una distribución de Wishart?

### 8. Comprueba por simulación que $n\mathbf{S}$, obtenida a partir de muestras de una normal multivariante, sigue aproximadamente una distribución de Wishart. ¿Coinciden las medias empíricas de los elementos de $n\mathbf{S}$ con los valores esperados según $n\boldsymbol{\Sigma}$? ¿Qué ocurre si se aumenta o reduce $n$? ¿Y si se cambia la matriz $\boldsymbol{\Sigma}$?

### 9. Genera una muestra de tamaño $n$ y dimensión $p$ de una distribución normal multivariante. Calcula el estadístico de Hotelling $T^2$ y transforma su valor a la escala de una F mediante la relación conocida entre ambas distribuciones. Explora cómo varía el valor de $T^2$ al modificar: el tamaño muestral $n$,la dimensión $p$,la distancia entre el vector de medias y un valor de referencia $\boldsymbol{\mu}_0$. Representa gráficamente el comportamiento de $T^2$ o de su equivalente en la escala F cuando cambias estos parámetros.

### 10. Simula ahora varios grupos con medias distintas pero con la misma matriz de covarianzas, y calcula el valor de Wilks $\Lambda$ y su transformación a F. Usa el siguiente código como base:

```{r}
library(MASS)
# Ejemplo con 3 grupos simulados
set.seed(123)
g <- 3; n <- 25; p <- 2
Sigma <- matrix(c(1, 0.6, 0.6, 1), 2)
mu_list <- list(c(0,0), c(1,0.5), c(0.5,1))
X <- do.call(rbind, lapply(1:g, function(k) mvrnorm(n, mu_list[[k]], Sigma)))
grupo <- factor(rep(1:g, each = n))
D <- data.frame(grupo, X)

# MANOVA (acrónimo de Multivariate Analysis of Variance) es la extensión multivariante del ANOVA
modelo <- manova(cbind(X1, X2) ~ grupo, data = D)
summary(modelo, test = "Wilks")
```

##### 10a. Calcula manualmente las matrices $\mathbf{W}$, $\mathbf{B}$ y $\mathbf{T}$, y el cociente $\Lambda = |\mathbf{W}| / |\mathbf{T}|$.

##### 10b. Comprueba que la F obtenida en la salida de summary(modelo, test = "Wilks") coincide con la transformación aproximada de $\Lambda$.

##### 10c. Repite la simulación con medias más separadas y observa cómo varían $\Lambda$ y F.
